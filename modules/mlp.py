# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ArthurSiqueiraS/TCC/blob/master/nn/MLP.ipynb
"""

import torch
import torch.nn as nn
from random import shuffle

import numpy as np
import pandas as pd
from fuzzywuzzy import fuzz
from sklearn.model_selection import train_test_split

def load_dataset():
    dataset = pd.read_csv('https://raw.githubusercontent.com/ArthurSiqueiraS/TCC/master/nn/dataset.csv?token=AHJJ64SFUHBCQXXXJZRMX5C52AADW')
    return dataset.fillna('')

def get_fuzz_ratios(left, right, min=None):
    if not (bool(left) and bool(right)):
        return [0., 0.]
    
    ratio = fuzz.ratio(left, right)
    token_set_ratio = fuzz.token_set_ratio(left, right)
    if min:
        if ratio < min:
            ratio = 0
        if token_set_ratio < min:
            ratio = 0

    return [
        ratio/100.0,
        token_set_ratio/100.0,
    ]
    
def get_row_productions(row, columns):
    left, right = {}, {}
    for field in columns:
        if 'p1.' in field:
            left[field[3:]] = row[field]
        elif 'p2.' in field:
            right[field[3:]] = row[field]

    return left, right

def get_features(left, right):
    features = [
        # left['categoria'] == right['categoria'],
        # left['subcategoria'] == right['subcategoria'],
        abs(int(left['ano']) - int(right['ano']))/10.,    
    ]
    features += get_fuzz_ratios(left['titulo_normalizado_2'], right['titulo_normalizado_2'])
    features += get_fuzz_ratios(left['doi'], right['doi'], 90)
    features += get_fuzz_ratios(left['autores_normalizados'][:500], right['autores_normalizados'][:500])
    features += get_fuzz_ratios(str(left['id_estudante']), str(right['id_estudante']), 90)
    features += get_fuzz_ratios(str(left['id_orientador']), str(right['id_orientador']), 90)
    features += get_fuzz_ratios(left['evento'], right['evento'])
    features += get_fuzz_ratios(left['issn'], right['issn'], 90)
    features += get_fuzz_ratios(left['isbn'], right['isbn'], 90)
    return np.array(features)/1.0

def to_features_df(df):
    features_df = []
    
    for (i, row) in df.iterrows():
        left, right = get_row_productions(row, df.columns)
        features_df.append(get_features(left, right))

    return features_df

def load_train_test_dataset(test_size=0.30):
    dataset = load_dataset()
    features_dataset = to_features_df(dataset)
    labels = np.array(dataset['Match'].values) * 1

    return train_test_split(features_dataset, labels, test_size=0.20)

class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(17, 32)
        self.fc2 = nn.Linear(32, 2)
        self.activation_function = nn.Sigmoid()

    def forward(self, x):
        x = self.activation_function(self.fc1(x))
        x = self.activation_function(self.fc2(x))
        return x

def train(epochs=10000):
    losses = []
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}:")
        shuffle(train_dataset)

        running_loss = 0

        for inputs, labels in train_dataset:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            
            outputs = model(inputs)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()

            _, pred = torch.max(outputs.data, 1)

            running_loss += loss.item()

        print(running_loss/len(train_dataset))

if __name__ == '__main__':
    X_train, X_test, y_train, y_test = load_train_test_dataset(0.25)

    indices = len(X_train)/200

    X_train_tensor = np.array_split(torch.FloatTensor(X_train), indices)
    y_train_tensor = np.array_split(torch.tensor(y_train), indices)
    X_test_tensor = np.array_split(torch.FloatTensor(X_test), indices)
    y_test_tensor = np.array_split(torch.tensor(y_test), indices)

    train_dataset = list(zip(X_train_tensor, y_train_tensor))
    test_dataset = list(zip(X_test_tensor, y_test_tensor))

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    model = MLP()
    model.to(device)

    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.001)
    loss_fn = nn.CrossEntropyLoss()
            
    train()

    correct = 0
    total = len(test_dataset)
    with torch.no_grad():
        for inputs, labels in test_dataset:
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            _, pred = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (pred == labels).sum().item()
        
    print(f"accuracy: {100 * correct / total}")

    dataset = load_dataset()

    wrong = 0
    for i, row in dataset.iterrows():
        left, right = get_row_productions(row, dataset.columns)
        features = torch.FloatTensor([get_features(left, right)])
        output = model(features)
        label = True if output[0][0] < output[0][1] else False
        if label != row['Match']:
            wrong += 1

    print(f"{wrong} errados")
            
    torch.save(model.state_dict(), './production_matcher')

